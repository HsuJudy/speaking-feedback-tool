name: ML Training Pipeline

on:
  schedule:
    # Run every Sunday at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      model_type:
        description: 'Type of model to train'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - sentiment
        - emotion
        - custom
  push:
    paths:
      - 'data/**'
      - 'models/**'
      - 'train.py'
      - 'train_custom_models.py'

env:
  WANDB_PROJECT: speaking-feedback-tool
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}

jobs:
  # ========================================
  # DATA VALIDATION
  # ========================================
  validate-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Validate data integrity
      run: |
        python demo_hash_validation.py --validate-all
        
    - name: Check data splits
      run: |
        python demo_multi_dataset_management.py --validate-splits
        
    - name: Upload data validation report
      uses: actions/upload-artifact@v3
      with:
        name: data-validation-report
        path: data_validation_report.json

  # ========================================
  # MODEL TRAINING
  # ========================================
  train-models:
    needs: validate-data
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model_type: [sentiment, emotion, custom]
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Set up W&B
      env:
        WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
      run: |
        wandb login $WANDB_API_KEY
        
    - name: Train ${{ matrix.model_type }} model
      run: |
        if [ "${{ matrix.model_type }}" = "sentiment" ]; then
          python train.py --model-type sentiment
        elif [ "${{ matrix.model_type }}" = "emotion" ]; then
          python train.py --model-type emotion
        elif [ "${{ matrix.model_type }}" = "custom" ]; then
          python train_custom_models.py
        fi
        
    - name: Log model to W&B
      run: |
        python utils/wandb_utils.py --log-model ${{ matrix.model_type }}
        
    - name: Log model to MLflow
      run: |
        python mlflow_integration.py --log-model ${{ matrix.model_type }}
        
    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: ${{ matrix.model_type }}-model
        path: |
          models/custom/*.pkl
          models/custom/*.json
          *.h5
          *.pth

  # ========================================
  # MODEL EVALUATION
  # ========================================
  evaluate-models:
    needs: train-models
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        path: models/
        
    - name: Evaluate models
      run: |
        python test_models.py --evaluate-all
        
    - name: Generate evaluation report
      run: |
        python demo_custom_models.py --evaluate
        
    - name: Upload evaluation report
      uses: actions/upload-artifact@v3
      with:
        name: model-evaluation-report
        path: evaluation_report.json

  # ========================================
  # MODEL DEPLOYMENT
  # ========================================
  deploy-models:
    needs: evaluate-models
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        path: models/
        
    - name: Deploy to MLflow Model Registry
      run: |
        python mlflow_integration.py --deploy-models
        
    - name: Deploy to Triton
      run: |
        python triton_integration.py --deploy-all
        
    - name: Update model serving
      run: |
        python demo_nvidia_mlops.py --update-serving
        
    - name: Test deployed models
      run: |
        python test_models.py --test-deployed

  # ========================================
  # PERFORMANCE MONITORING
  # ========================================
  monitor-performance:
    needs: deploy-models
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run performance tests
      run: |
        python test_models.py --performance
        
    - name: Generate performance report
      run: |
        python sentiment_observability_pipeline.py --performance-report
        
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: performance_report.json

  # ========================================
  # NOTIFICATIONS
  # ========================================
  notify-completion:
    needs: [monitor-performance]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Check training status
      id: check-status
      run: |
        if [ "${{ needs.monitor-performance.result }}" = "success" ]; then
          echo "status=success" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
        fi
        
    - name: Send success notification
      if: steps.check-status.outputs.status == 'success'
      run: |
        echo "✅ ML Training Pipeline completed successfully!"
        echo "📊 Models deployed and ready for inference"
        echo "📈 Performance monitoring active"
        
    - name: Send failure notification
      if: steps.check-status.outputs.status == 'failure'
      run: |
        echo "❌ ML Training Pipeline failed!"
        echo "🔍 Check the logs for details"
        exit 1 