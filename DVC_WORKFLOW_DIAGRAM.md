# DVC (Data Version Control) Workflow Diagram

## ðŸ”„ Complete DVC Workflow

```mermaid
graph TD
    A[ðŸ“ Raw Dataset] --> B[ðŸ” DVC Version Control]
    B --> C[ðŸ“Š Generate Content Hash]
    C --> D[ðŸ’¾ Store in data_versions.json]
    D --> E[ðŸ”„ Version Tracking]
    
    E --> F[âœ… Integrity Verification]
    E --> G[ðŸ‘¥ Team Collaboration]
    E --> H[ðŸ”¬ Experiment Reproducibility]
    
    F --> I[ðŸ“ˆ Model Training]
    G --> J[ðŸ”„ Data Sharing]
    H --> K[ðŸ“‹ Audit Trail]
    
    I --> L[ðŸ¤– Model Deployment]
    J --> M[ðŸ”’ Data Lineage]
    K --> N[ðŸ“Š MLOps Pipeline]
```

## ðŸ“Š Data Versioning Process

```mermaid
sequenceDiagram
    participant U as User
    participant DVC as DataVersionControl
    participant FS as File System
    participant VF as Version File
    
    U->>DVC: version_dataset(file_path, name, description)
    DVC->>FS: Read dataset file
    FS-->>DVC: File content
    DVC->>DVC: Calculate SHA256 hash
    DVC->>VF: Store metadata + hash
    VF-->>DVC: Confirmation
    DVC-->>U: Return hash (expected_hash)
    
    Note over U,DVC: Later: Verify integrity
    U->>DVC: verify_dataset_integrity(file_path, expected_hash)
    DVC->>FS: Read current file
    FS-->>DVC: Current content
    DVC->>DVC: Calculate current hash
    DVC->>DVC: Compare with expected_hash
    DVC-->>U: Return verification result
```

## ðŸ—ï¸ Project Structure with DVC

```
Speaking Feedback Tool/
â”œâ”€â”€ ðŸ“ data/
â”‚   â”œâ”€â”€ ðŸ“„ data_versions.json          # DVC Version Registry
â”‚   â”œâ”€â”€ ðŸ“ raw/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ emotion_dataset.csv     # Versioned Dataset
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ audio_features.csv      # Versioned Dataset
â”‚   â”‚   â””â”€â”€ ðŸ“„ validation_data.csv     # Versioned Dataset
â”‚   â”œâ”€â”€ ðŸ“ processed/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ emotion_processed.csv   # Versioned Processed Data
â”‚   â”‚   â””â”€â”€ ðŸ“„ audio_processed.csv     # Versioned Processed Data
â”‚   â””â”€â”€ ðŸ“ splits/
â”‚       â”œâ”€â”€ ðŸ“ emotion_classification/
â”‚       â”‚   â”œâ”€â”€ ðŸ“„ train.csv          # Versioned Split
â”‚       â”‚   â”œâ”€â”€ ðŸ“„ val.csv            # Versioned Split
â”‚       â”‚   â””â”€â”€ ðŸ“„ test.csv           # Versioned Split
â”‚       â””â”€â”€ ðŸ“ validation_testing/
â”‚           â”œâ”€â”€ ðŸ“„ train.csv          # Versioned Split
â”‚           â”œâ”€â”€ ðŸ“„ val.csv            # Versioned Split
â”‚           â””â”€â”€ ðŸ“„ test.csv           # Versioned Split
â”œâ”€â”€ ðŸ“ models/
â”‚   â”œâ”€â”€ ðŸ“„ model_registry.json        # Model Version Registry
â”‚   â””â”€â”€ ðŸ“ custom/
â”‚       â”œâ”€â”€ ðŸ“„ random_forest_emotion_model.pkl
â”‚       â”œâ”€â”€ ðŸ“„ logistic_regression_emotion_model.pkl
â”‚       â””â”€â”€ ðŸ“„ gradient_boosting_emotion_model.pkl
â””â”€â”€ ðŸ“ pipeline_results/
    â”œâ”€â”€ ðŸ“„ pipeline_results_conformer_ctc.json
    â”œâ”€â”€ ðŸ“„ pipeline_results_video_sentiment.json
    â””â”€â”€ ðŸ“„ pipeline_results_audio_emotion.json
```

## ðŸ”‘ Key DVC Concepts

### 1. Content Hashing
```
Input: emotion_dataset.csv (1,000 samples)
Process: SHA256(content)
Output: 9fd78ddf1b085c9dc1caa5451570e12e497ad9360fa6bd00f6e25d300d212203
```

### 2. Version Registry Structure
```json
{
  "9fd78ddf1b085c9d...": {
    "dataset_name": "emotion_classification_data",
    "file_path": "data/raw/emotion_dataset.csv",
    "content_hash": "9fd78ddf1b085c9dc1caa5451570e12e497ad9360fa6bd00f6e25d300d212203",
    "description": "Initial version of emotion classification dataset",
    "tags": ["emotion", "audio", "classification"],
    "created_at": "2025-08-01T15:23:54.265372",
    "file_size": 4153,
    "file_modified": "2025-08-01T15:23:54.265372"
  }
}
```

## ðŸ”„ DVC Workflow States

```mermaid
stateDiagram-v2
    [*] --> RawData: Create dataset
    RawData --> Versioned: dvc.version_dataset()
    Versioned --> Verified: dvc.verify_dataset_integrity()
    Verified --> Training: Use in experiments
    Training --> ModelDeployed: Deploy model
    ModelDeployed --> [*]
    
    Versioned --> Modified: Data changes
    Modified --> ReVersioned: Re-version dataset
    ReVersioned --> Verified
```

## ðŸ‘¥ Team Collaboration Flow

```mermaid
graph LR
    A[ðŸ‘¤ Team Member A] --> B[ðŸ“¤ Share Dataset + Hash]
    B --> C[ðŸ‘¤ Team Member B]
    C --> D[ðŸ” Verify Integrity]
    D --> E[âœ… Use Verified Dataset]
    E --> F[ðŸ¤– Train Model]
    F --> G[ðŸ“Š Reproduce Results]
```

## ðŸ”¬ Experiment Reproducibility

```mermaid
graph TD
    A[ðŸ“‹ Experiment Metadata] --> B[ðŸ”‘ Dataset Hash]
    B --> C[ðŸ” Load Expected Hash]
    C --> D[ðŸ“ Locate Dataset File]
    D --> E[âœ… Verify Integrity]
    E --> F[ðŸ¤– Run Experiment]
    F --> G[ðŸ“Š Reproduce Results]
    
    E --> H{Verification Failed?}
    H -->|Yes| I[âŒ Stop Experiment]
    H -->|No| F
```

## ðŸ“ˆ MLOps Integration

```mermaid
graph TD
    A[ðŸ“Š Data Pipeline] --> B[ðŸ” DVC Versioning]
    B --> C[ðŸ¤– Model Training]
    C --> D[ðŸ“‹ Model Registry]
    D --> E[ðŸš€ Model Deployment]
    E --> F[ðŸ“Š Monitoring]
    F --> G[ðŸ”„ Data Drift Detection]
    G --> H[ðŸ“ˆ Retraining Trigger]
    H --> A
```

## ðŸŽ¯ Real-World Scenarios

### Scenario 1: New Dataset Creation
```
1. ðŸ“ Create/obtain dataset file
2. ðŸ” Call dvc.version_dataset()
3. ðŸ”‘ Store returned hash as 'expected_hash'
4. ðŸ¤– Use hash in experiments
```

### Scenario 2: Experiment Reproducibility
```
1. ðŸ“‹ Load expected hash from experiment metadata
2. ðŸ” Call dvc.verify_dataset_integrity()
3. âœ… Only proceed if verification passes
4. ðŸ¤– Run experiment with verified dataset
```

### Scenario 3: Team Collaboration
```
1. ðŸ“¤ Share dataset file with team
2. ðŸ”‘ Share expected hash with team
3. ðŸ” Each team member verifies integrity
4. âœ… All use identical dataset version
```

### Scenario 4: Model Deployment
```
1. ðŸ“‹ Store dataset hash with model metadata
2. ðŸ” Verify dataset integrity before deployment
3. ðŸš€ Deploy model with verified data
4. ðŸ“Š Track data lineage for audit
```

## ðŸ” Integrity Verification Process

```mermaid
flowchart TD
    A[ðŸ“ Dataset File] --> B[ðŸ” Calculate Current Hash]
    B --> C[ðŸ”‘ Expected Hash]
    C --> D{Hash Match?}
    D -->|Yes| E[âœ… Integrity Verified]
    D -->|No| F[âŒ Integrity Failed]
    E --> G[ðŸ¤– Proceed with Experiment]
    F --> H[ðŸ›‘ Stop Process]
```

## ðŸ“Š Benefits of DVC

### âœ… Data Integrity
- **Prevents silent corruption**: Hash verification catches data changes
- **Ensures reproducibility**: Same hash = same data
- **Provides audit trail**: Track all dataset versions

### âœ… Team Collaboration
- **Centralized versioning**: One source of truth
- **Easy sharing**: Share hash + file
- **Consistent data**: All team members use identical datasets

### âœ… MLOps Best Practices
- **Model versioning**: Link models to specific dataset versions
- **Experiment tracking**: Store dataset hashes with experiments
- **Deployment safety**: Verify data integrity before deployment

### âœ… Compliance & Audit
- **Data lineage**: Track which dataset version was used
- **Reproducibility**: Recreate exact experiment conditions
- **Audit trail**: Complete history of data changes

## ðŸš€ Next Steps

1. **ðŸ“Š Real Data Integration**: Replace dummy data with actual datasets
2. **ðŸ”— CI/CD Integration**: Automate DVC verification in pipelines
3. **ðŸ“ˆ Advanced Monitoring**: Set up alerts for data drift
4. **ðŸ‘¥ Team Workflows**: Implement proper data sharing protocols
5. **ðŸ”’ Security**: Add encryption for sensitive datasets

---

*This diagram shows how DVC integrates with the Speaking Feedback Tool to provide robust data versioning, integrity verification, and team collaboration capabilities.* 